{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load testing set from nodesData.csv and select feature that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'admin', 'changedAt', 'children', 'childrenLength', 'choices',\n",
      "       'choicesLength', 'closedHeight', 'comments', 'content', 'corrects',\n",
      "       'createdAt', 'deleted', 'height', 'maxVersionRating', 'nodeImage',\n",
      "       'nodeType', 'parents', 'parentsLength', 'references',\n",
      "       'referencesLength', 'studied', 'tags', 'tagsLength', 'title',\n",
      "       'updatedAt', 'versions', 'viewers', 'wrongs'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>children</th>\n",
       "      <th>parents</th>\n",
       "      <th>height</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059ccSkCvCppWgGPQ9G</td>\n",
       "      <td>[{\"node\":\"GtyDY4Dtl1loG0CNy5Lz\",\"title\":\"Metho...</td>\n",
       "      <td>[{\"title\":\"Further approaches that could lead ...</td>\n",
       "      <td>210.0</td>\n",
       "      <td>Handling more complicated context\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00NwvYhgES9mjNQ9LRhG</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"title\":\"Drugs working to inactivate transmi...</td>\n",
       "      <td>147.0</td>\n",
       "      <td>Cocaine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00bwtgMsbiN0B5HkAxfE</td>\n",
       "      <td>[{\"node\":\"DUmRznFNxDg3tz2180xv\",\"label\":\"\",\"ti...</td>\n",
       "      <td>[{\"node\":\"PDQ3oqH0iMXh74xNdLLn\",\"title\":\"Emoti...</td>\n",
       "      <td>125.0</td>\n",
       "      <td>Testing Emotional Regulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00mgqGYES5nr2AXUchws</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"label\":\"\",\"node\":\"wot1mVqtGrFP82NtvnZQ\",\"ti...</td>\n",
       "      <td>252.0</td>\n",
       "      <td>Impact of the COVID‐19 pandemic on telehealth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00yXUI71Epq9q8b9xH9K</td>\n",
       "      <td>[{\"node\":\"748r8Mf8cjaxnVh6VKns\",\"title\":\"Refer...</td>\n",
       "      <td>[{\"label\":\"\",\"node\":\"x5zFnnaaioBUd4iU2Y4w\",\"ti...</td>\n",
       "      <td>367.0</td>\n",
       "      <td>First Positive Cases in Cats and Dogs in USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36750</th>\n",
       "      <td>zzS0u9ZJD1hbIG3b7TiH</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"label\":\"\",\"title\":\"Detection Methods Used C...</td>\n",
       "      <td>220.0</td>\n",
       "      <td>Detection Methods Used Currently by WHO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36751</th>\n",
       "      <td>zzWNrYf8GjnXBqkf9mJQ</td>\n",
       "      <td>[{\"title\":\"Methods for determining odds of abn...</td>\n",
       "      <td>[{\"title\":\"Findings opposed to use of Azithrom...</td>\n",
       "      <td>334.0</td>\n",
       "      <td>Proportion of abnormal ECG findings  in hospit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36752</th>\n",
       "      <td>zzeuylk97oZMOZ4p6tzQ</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"node\":\"qlTVhbjcDPlKuIxXFN1T\",\"title\":\"Niels...</td>\n",
       "      <td>199.0</td>\n",
       "      <td>NNG Reference Article: Usability Heuristic 8 -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36753</th>\n",
       "      <td>zzkoyR3Ke1kRaH5Pf9ZH</td>\n",
       "      <td>[{\"title\":\"COVID-19 and TB co-infection - 'Fin...</td>\n",
       "      <td>[{\"node\":\"HzLfRzANJBbX8nvkq1m5\",\"title\":\"Refer...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>Journal of Infection Papers Related to Seconda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36754</th>\n",
       "      <td>zzyy7iThmjX3M9OzngE8</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"title\":\"Subtitles\",\"label\":\"\",\"node\":\"BMsuu...</td>\n",
       "      <td>399.0</td>\n",
       "      <td>Sans serif for Subtitle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36755 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0      0059ccSkCvCppWgGPQ9G   \n",
       "1      00NwvYhgES9mjNQ9LRhG   \n",
       "2      00bwtgMsbiN0B5HkAxfE   \n",
       "3      00mgqGYES5nr2AXUchws   \n",
       "4      00yXUI71Epq9q8b9xH9K   \n",
       "...                     ...   \n",
       "36750  zzS0u9ZJD1hbIG3b7TiH   \n",
       "36751  zzWNrYf8GjnXBqkf9mJQ   \n",
       "36752  zzeuylk97oZMOZ4p6tzQ   \n",
       "36753  zzkoyR3Ke1kRaH5Pf9ZH   \n",
       "36754  zzyy7iThmjX3M9OzngE8   \n",
       "\n",
       "                                                children  \\\n",
       "0      [{\"node\":\"GtyDY4Dtl1loG0CNy5Lz\",\"title\":\"Metho...   \n",
       "1                                                     []   \n",
       "2      [{\"node\":\"DUmRznFNxDg3tz2180xv\",\"label\":\"\",\"ti...   \n",
       "3                                                     []   \n",
       "4      [{\"node\":\"748r8Mf8cjaxnVh6VKns\",\"title\":\"Refer...   \n",
       "...                                                  ...   \n",
       "36750                                                 []   \n",
       "36751  [{\"title\":\"Methods for determining odds of abn...   \n",
       "36752                                                 []   \n",
       "36753  [{\"title\":\"COVID-19 and TB co-infection - 'Fin...   \n",
       "36754                                                 []   \n",
       "\n",
       "                                                 parents  height  \\\n",
       "0      [{\"title\":\"Further approaches that could lead ...   210.0   \n",
       "1      [{\"title\":\"Drugs working to inactivate transmi...   147.0   \n",
       "2      [{\"node\":\"PDQ3oqH0iMXh74xNdLLn\",\"title\":\"Emoti...   125.0   \n",
       "3      [{\"label\":\"\",\"node\":\"wot1mVqtGrFP82NtvnZQ\",\"ti...   252.0   \n",
       "4      [{\"label\":\"\",\"node\":\"x5zFnnaaioBUd4iU2Y4w\",\"ti...   367.0   \n",
       "...                                                  ...     ...   \n",
       "36750  [{\"label\":\"\",\"title\":\"Detection Methods Used C...   220.0   \n",
       "36751  [{\"title\":\"Findings opposed to use of Azithrom...   334.0   \n",
       "36752  [{\"node\":\"qlTVhbjcDPlKuIxXFN1T\",\"title\":\"Niels...   199.0   \n",
       "36753  [{\"node\":\"HzLfRzANJBbX8nvkq1m5\",\"title\":\"Refer...   200.0   \n",
       "36754  [{\"title\":\"Subtitles\",\"label\":\"\",\"node\":\"BMsuu...   399.0   \n",
       "\n",
       "                                                   title  \n",
       "0                    Handling more complicated context\\n  \n",
       "1                                                Cocaine  \n",
       "2                          Testing Emotional Regulation   \n",
       "3      Impact of the COVID‐19 pandemic on telehealth ...  \n",
       "4           First Positive Cases in Cats and Dogs in USA  \n",
       "...                                                  ...  \n",
       "36750            Detection Methods Used Currently by WHO  \n",
       "36751  Proportion of abnormal ECG findings  in hospit...  \n",
       "36752  NNG Reference Article: Usability Heuristic 8 -...  \n",
       "36753  Journal of Infection Papers Related to Seconda...  \n",
       "36754                            Sans serif for Subtitle  \n",
       "\n",
       "[36755 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data='D:\\\\Career\\\\DS-ML\\\\1cademy\\\\research\\\\data\\\\testing\\\\'\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#load data\n",
    "os.chdir(path_data)\n",
    "df = pd.read_csv('nodesData.csv')\n",
    "print(df.columns) \n",
    "\n",
    "#select the columns we need \n",
    "df=df[['id','children','parents','height','title']]\n",
    "(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess dataframe to dictionary\n",
    "{_nodeCo3unt:int,\\\n",
    "_edgeCount: int,\\\n",
    "_nodes:dictionary{nodeKey:(nodeHeight,nodeWidth)}\\\n",
    "_children:dictionary{nodeKey:[childrenKey1, key2...]}\\\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29766\n",
      "30938\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "node_dic=dict()\n",
    "edge_dic=defaultdict(list)\n",
    "titles=set()\n",
    "num_node=0\n",
    "num_edge=0\n",
    "\n",
    "#add valid nodes to node_dic\n",
    "for i in range(len(df.id)):\n",
    "    if not np.isnan(df['height'][i]):\n",
    "        num_node+=1\n",
    "        node_dic[df['id'][i]]=(600,float(df['height'][i]))\n",
    "        titles.add(df['title'][i])\n",
    "\n",
    "#add valid node and children relation to edge dictionary\n",
    "for i in range(len(df.id)):\n",
    "    if not np.isnan(df['height'][i]):\n",
    "        data='g='+df['children'][i]\n",
    "        data=data.replace('true','True')\n",
    "        data=data.replace('false','False')\n",
    "        lcls = locals()\n",
    "        exec(data, globals(), lcls )\n",
    "        g = lcls[\"g\"]\n",
    "        for children in g:\n",
    "            if children['node'] in node_dic.keys():\n",
    "                num_edge+=1\n",
    "                edge_dic[df['id'][i]].append(children['node'])\n",
    "        \n",
    "print(num_node)\n",
    "print(num_edge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the function that process the output from dagre to DGL graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "def process_single_graph(data):#input is str directly read from one txt file\n",
    "    #-----------data cleaning and rearranging--------\n",
    "    data=data.replace('true','True')\n",
    "    data=data.replace('false','False')\n",
    "    data='g='+str(data)\n",
    "    lcls = locals()\n",
    "    exec(data, globals(), lcls )\n",
    "    g = lcls[\"g\"]\n",
    "    #print(g)\n",
    "    #------------------------ process to attributes\n",
    "    G = dgl.DGLGraph()\n",
    "    G.add_nodes(g['_nodeCount'])\n",
    "    #print(g['_nodeCount'])\n",
    "    nodes_ordered=[k for k, v in sorted(nodes.items(), key=lambda item: item[1])]\n",
    "    in_=[]\n",
    "    out_=[]\n",
    "    #print(len(g['_edgeObjs'].keys()))\n",
    "    for e in g['_edgeObjs'].keys():\n",
    "        out_.append(nodes[(g['_edgeObjs'][e]['v'][1:])])\n",
    "        in_.append(nodes[(g['_edgeObjs'][e]['w'][1:])])\n",
    "    out_=torch.tensor(out_,dtype=torch.long)\n",
    "    in_=torch.tensor(in_,dtype=torch.long)\n",
    "    G.add_edges(out_,in_)\n",
    "    #scale both length and position with output canvas width and height.\n",
    "    #This way, the trained model assume that the canvas is bounded, but the input size fit just right with canvas size\n",
    "    width=g['_label']['width']\n",
    "    height=g['_label']['height']\n",
    "\n",
    "    #scale both length and position with output canvas width and height.\n",
    "    #This way, the trained model assume that the canvas is boundless\n",
    "    width=50\n",
    "    height=50\n",
    "    x_=[(g['_nodes']['c'+node]['width']/width,g['_nodes']['c'+node]['height']/height) for node in nodes_ordered]\n",
    "    y_=[(g['_nodes']['c'+node]['x']/width,g['_nodes']['c'+node]['y']/height) for node in nodes_ordered]\n",
    "    \n",
    "    y_=list(itertools.chain.from_iterable(y_))\n",
    "    #y_=torch.tensor(y_,dtype=torch.double)\n",
    "    #print(y_)\n",
    "    G.ndata['x']=torch.tensor(x_,dtype=torch.double)\n",
    "    try:\n",
    "        n_edge=g['_edgeCount']\n",
    "    except:\n",
    "        n_edge=0\n",
    "    G.edata['x']=torch.tensor(np.zeros((n_edge,1)),dtype=torch.double)\n",
    "    return (G,y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulate the way user extract subgraphs, sample subgraph, process it using dagre and DGL\n",
    "1. select a fix number of inital nodes, range from 1 to 10, uniform dist.\n",
    "2. for each initial nodes, select certain number of children nodes.\n",
    "3. for the children selected from 2, select certain percentage of children nodes\n",
    "4. repeat step 3 for K times as long as there'are still children nodes. K is random number from 2 to 10, uniform dist.\n",
    "5. After finish sampling a subgraph, save it as JSON file, process it using dagre module in javascript and get node positions, reload the dagre-processed graph and format it using DGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [15:30:26<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('D:\\\\Career\\\\DS-ML\\\\1cademy\\\\research\\\\data\\\\testing\\\\')\n",
    "num_samples=100000\n",
    "nodes_wl_child=list(edge_dic.keys())\n",
    "graphs = []\n",
    "labels = []\n",
    "times=[]\n",
    "\n",
    "random.seed(911)#10,22,31，98\n",
    "for i in tqdm(range(num_samples)):\n",
    "    #if i%1000==0:print(i)\n",
    "    num_seed_node=int(np.random.uniform(0,1)*(15+1-7))+7\n",
    "    K=int(np.random.uniform(0,1)*(10+1-3))+3\n",
    "    nodes=dict()\n",
    "    edges=dict()\n",
    "    dag=dict({'nodes':list(),'edges':list()})\n",
    "    idx=0\n",
    "    nodes_cur=random.sample(list(node_dic),num_seed_node)\n",
    "    G = dgl.DGLGraph()\n",
    "    for j in range(K):\n",
    "        for node in nodes_cur.copy():\n",
    "            #print(j)\n",
    "            nodes_cur.pop(0)\n",
    "            if node not in nodes.keys():\n",
    "                nodes[node]=idx\n",
    "                dag['nodes'].append({'label':'c'+node,'width':node_dic[node][0],'height':node_dic[node][1]})\n",
    "                idx+=1\n",
    "                num_children=len(edge_dic[node])\n",
    "                if num_children>5:\n",
    "                    num_children=int(np.random.uniform(0,1)*(5+1-3))+3\n",
    "                elif num_children==0:\n",
    "                    continue\n",
    "                children=random.sample(edge_dic[node],num_children)\n",
    "                #children=edge_dic[node]\n",
    "                nodes_cur+=children\n",
    "                edges[node]=children\n",
    "                dag['edges']+=[{'parent':'c'+node,'child':'c'+child} for child in children]\n",
    "    for node in nodes_cur:\n",
    "        if node not in nodes.keys():\n",
    "            dag['nodes'].append({'label':'c'+node,'width':float(node_dic[node][0]),'height':float(node_dic[node][1])})\n",
    "            nodes[node]=idx\n",
    "            idx+=1\n",
    "    \n",
    "    #there is no duplicate in nodes and dag['nodes'], but there could be duplicates in dag['edges'], and it could be 0\n",
    "    l=[(i['parent'],i['child']) for i in dag['edges']]\n",
    "    dag['n_nodes']=len(dag['nodes'])\n",
    "    dag['n_edges']=len(Counter(l))\n",
    "    with open('data.json', 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(dag, f)  \n",
    "    subprocess.run('node index.js',shell=True, capture_output=True)\n",
    "    filename='graph.txt'\n",
    "    text_file = open(filename, \"r\")\n",
    "    data = text_file.read()\n",
    "    text_file.close()\n",
    "    filename='time.txt'\n",
    "    text_file = open(filename, \"r\")\n",
    "    time = text_file.read()\n",
    "    text_file.close()\n",
    "    time=(time.split(','))\n",
    "    #----------process and add to dataset\n",
    "    (g,label)=process_single_graph(data)\n",
    "    graphs.append(g)\n",
    "    labels.append(label)\n",
    "    times.append(time)\n",
    "\n",
    "def trim(y):\n",
    "    y_len=[len(_) for _ in y]\n",
    "    y_dim=max(y_len)\n",
    "    y_dim=500\n",
    "    yy=([_+[0 for _ in range(y_dim-len(_))] for _ in y])\n",
    "    return yy\n",
    "labels=trim(labels)\n",
    "#remove graphs that have too many nodes\n",
    "n=len(labels)\n",
    "for i in range(n-1,-1,-1):\n",
    "    if len(labels[i])>500:\n",
    "        graphs.pop(i)\n",
    "        labels.pop(i)\n",
    "        times.pop(i)\n",
    "labels=torch.tensor(labels,dtype=torch.double)\n",
    "\n",
    "\n",
    "filehandler = open(\"graphs5.obj\",\"wb\")\n",
    "pickle.dump(graphs,filehandler)\n",
    "filehandler.close()\n",
    "filehandler = open(\"labels5.obj\",\"wb\")\n",
    "pickle.dump(labels,filehandler)\n",
    "filehandler.close()\n",
    "filehandler = open(\"times5.obj\",\"wb\")\n",
    "pickle.dump(times,filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "\n",
    "#print(dag['n_nodes'])\n",
    "#print(dag['n_edges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes below are tunning codes (don't run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-31dc83dde878>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mg1\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mg2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mg3\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mg4\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mg5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mfilehandler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"graphs.obj\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilehandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mfilehandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import dgl\n",
    "os.chdir('D:\\\\Career\\\\DS-ML\\\\1cademy\\\\research\\\\data\\\\testing\\\\')\n",
    "with open(\"graphs1.obj\", \"rb\") as f:\n",
    "    rawdata = f.read()\n",
    "g1 = pickle.loads(rawdata)\n",
    "with open(\"graphs2.obj\", \"rb\") as f:\n",
    "    rawdata = f.read()\n",
    "g2 = pickle.loads(rawdata)\n",
    "with open(\"graphs3.obj\", \"rb\") as f:\n",
    "    rawdata = f.read()\n",
    "g3 = pickle.loads(rawdata)\n",
    "with open(\"graphs4.obj\", \"rb\") as f:\n",
    "    rawdata = f.read()\n",
    "g4 = pickle.loads(rawdata)\n",
    "with open(\"graphs5.obj\", \"rb\") as f:\n",
    "    rawdata = f.read()\n",
    "g5 = pickle.loads(rawdata)\n",
    "\n",
    "g1+=g2+g3+g4+g5\n",
    "filehandler = open(\"graphs.obj\",\"wb\")\n",
    "pickle.dump(g1,filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489723\n"
     ]
    }
   ],
   "source": [
    "print(len(g1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-daae380e4109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfilehandler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"graphs.obj\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilehandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfilehandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filehandler = open(\"graphs.obj\",\"wb\")\n",
    "pickle.dump(g1,filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 1), dtype=float64)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#process_single_graph(data)\n",
    "#print(dag['edges'])\n",
    "print(dag['n_edges'])\n",
    "#process_single_graph(data)\n",
    "print([(i['child'],i['parent']) in list(Counter(l).keys()) for i in dag['edges']])\n",
    "#print(list(Counter(l).keys()))\n",
    "#print(nodes_cur)\n",
    "#c8WDa0mv5yasgPPPqqW34\n",
    "np.zeros((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"label\": \"A\",\n",
      "      \"attributes\": {\n",
      "        \"width\": 200,\n",
      "        \"height\": 100\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"B\",\n",
      "      \"attributes\": {\n",
      "        \"width\": 300,\n",
      "        \"height\": 100\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"edges\": [\n",
      "    {\n",
      "      \"source\": \"A\",\n",
      "      \"target\": \"B\",\n",
      "      \"label\": \"edge1\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import dagre_py\n",
    "import json\n",
    "import os\n",
    "import webbrowser\n",
    "from typing import Dict, List\n",
    "\n",
    "import pkg_resources\n",
    "\n",
    "serve_dir = 'D:\\\\Career\\\\DS-ML\\\\1cademy\\\\research\\\\data\\\\testing\\\\'\n",
    "json_encoder=None\n",
    "with open(os.path.join(serve_dir, \"data.js\"), \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"let data = {}\".format(json.dumps(dagre_data, cls=json_encoder, indent=2)))\n",
    "with open('data.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(dagre_data, f)\n",
    "print((json.dumps(dagre_data, cls=json_encoder, indent=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"node\":\"GtyDY4Dtl1loG0CNy5Lz\",\"title\":\"Methods for handling more complicated context in RE\",\"label\":\"\"}]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_processed=\"D:\\\\Career\\\\DS-ML\\\\1cademy\\\\research\\\\data\\\\testing\\\\\"\n",
    "\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "import zipfile\n",
    "\n",
    "def process_single_graph(data):#input is str directly read from one txt file\n",
    "    #-----------data cleaning and rearranging--------\n",
    "    data=data.replace('true','True')\n",
    "    data=data.replace('false','False')\n",
    "    data='g='+str(data)\n",
    "    lcls = locals()\n",
    "    exec(data, globals(), lcls )\n",
    "    g = lcls[\"g\"]\n",
    "    #------------------------ process to attributes\n",
    "    G = dgl.DGLGraph()\n",
    "    G.add_nodes(g['_nodeCount'])\n",
    "    in_=[]\n",
    "    out_=[]\n",
    "    for e in g['_edgeObjs'].keys():\n",
    "        out_.append(int(g['_edgeObjs'][e]['v'][1:]))\n",
    "        in_.append(int(g['_edgeObjs'][e]['w'][1:]))\n",
    "    out_=torch.tensor(out_,dtype=torch.long)\n",
    "    in_=torch.tensor(in_,dtype=torch.long)\n",
    "    G.add_edges(out_,in_)\n",
    "    #scale both length and position with output canvas width and height.\n",
    "    #This way, the trained model assume that the canvas is bounded, but the input size fit just right with canvas size\n",
    "    width=g['_label']['width']\n",
    "    height=g['_label']['height']\n",
    "\n",
    "    #scale both length and position with output canvas width and height.\n",
    "    #This way, the trained model assume that the canvas is boundless\n",
    "    width=1500\n",
    "    height=1500\n",
    "\n",
    "    x_=[(g['_nodes']['n'+str(n)]['width']/width,g['_nodes']['n'+str(n)]['height']/height) for n in range(g['_nodeCount'])]\n",
    "    #for xx in x_:\n",
    "    #  if int(xx[1])==0:\n",
    "    #    print((\"degenerate box\",i,j))\n",
    "    #    print(x_)\n",
    "    #    break\n",
    "    #x_=[(g['_nodes']['n'+str(n)]['width'],g['_nodes']['n'+str(n)]['height']) for n in range(g['_nodeCount'])]\n",
    "    y_=[(g['_nodes']['n'+str(n)]['x']/width,g['_nodes']['n'+str(n)]['y']/height) for n in range(g['_nodeCount'])]\n",
    "    #y_=[(g['_nodes']['n'+str(n)]['x'],g['_nodes']['n'+str(n)]['y']) for n in range(g['_nodeCount'])]\n",
    "\n",
    "    y_=list(itertools.chain.from_iterable(y_))\n",
    "    #y_=torch.tensor(y_,dtype=torch.double)\n",
    "    #print(y_)\n",
    "    G.ndata['x']=torch.tensor(x_,dtype=torch.double)\n",
    "    G.edata['x']=torch.tensor(np.zeros((g['_edgeCount'],1)),dtype=torch.double)\n",
    "    return (G,y_)\n",
    "\n",
    "#--------iterate over all files to construct graph dataset--------\n",
    "sfix=['']+[' ('+str(i+1)+')' for i in range(n_zip)]#filename suffix\n",
    "graphs = []\n",
    "labels = []\n",
    "times=[]\n",
    "ct=0\n",
    "os.chdir(path_data1)\n",
    "for i in sfix:\n",
    "    zipname='training'+i+'.zip'\n",
    "    archive = zipfile.ZipFile(zipname, 'r')\n",
    "    for j in range(n_inzip):\n",
    "        ct+=1\n",
    "        #-----------load data----\n",
    "        filename='graph'+str(j)+'.txt'\n",
    "        data = (archive.read(filename).decode(\"utf-8\"))\n",
    "        filename='time'+str(j)+'.txt'\n",
    "        time = (archive.read(filename).decode(\"utf-8\"))\n",
    "        time=(time.split(','))\n",
    "        #----------process and add to dataset\n",
    "        (g,label)=process_single_graph(data)\n",
    "        graphs.append(g)\n",
    "        labels.append(label)\n",
    "        times.append(time)\n",
    "# make sure all sublists have same dimension and convert it torch tensor\n",
    "# Convert the label list to tensor for saving.\n",
    "def trim(y):\n",
    "    y_len=[len(_) for _ in y]\n",
    "    y_dim=max(y_len)\n",
    "    y_dim=256\n",
    "    yy=([_+[0 for _ in range(y_dim-len(_))] for _ in y])\n",
    "    return yy\n",
    "labels=trim(labels)\n",
    "labels=torch.tensor(labels,dtype=torch.double)\n",
    "\n",
    "os.chdir(path_data2)\n",
    "filehandler = open(\"graphs.obj\",\"wb\")\n",
    "pickle.dump(graphs,filehandler)\n",
    "filehandler.close()\n",
    "filehandler = open(\"labels.obj\",\"wb\")\n",
    "pickle.dump(labels,filehandler)\n",
    "filehandler.close()\n",
    "filehandler = open(\"times.obj\",\"wb\")\n",
    "pickle.dump(times,filehandler)\n",
    "filehandler.close()\n",
    "print(times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_epoch=25\n",
    "num_epochs=80\n",
    "lr_sched_step = lambda epoch: 1 - max(0, epoch - decay_epoch) / (num_epochs - decay_epoch)\n",
    "\n",
    "for i in range(80):\n",
    "    (lr_sched_step(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d1686ac76c73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0m_discriminator_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-d1686ac76c73>\u001b[0m in \u001b[0;36m_discriminator_loss\u001b[1;34m(logits, labels)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_discriminator_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3130\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3132\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _discriminator_loss(logits, labels):\n",
    "    return torch.mean(F.binary_cross_entropy_with_logits(logits, labels, reduction='none'))\n",
    "\n",
    "pred=torch.tensor([100,200,300])\n",
    "_discriminator_loss(pred, torch.ones_like(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-549f2303ac29>:24: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'getsizeof' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-549f2303ac29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_rec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'getsizeof' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def int64_feature(value):\n",
    "    if type(value) != list:\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def float_feature(value):\n",
    "    if type(value) != list:\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "writer = tf.python_io.TFRecordWriter('file.tfrecords')\n",
    "bytes = np.array(1.1).tostring() \n",
    "int = 1\n",
    "float = 1.1\n",
    "example = tf.train.Example(features=tf.train.Features(feature={'1': float_feature(float)}))\n",
    "writer.write(example.SerializeToString())\n",
    "writer.close()\n",
    "\n",
    "for str_rec in tf.python_io.tf_record_iterator('file.tfrecords'):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(str_rec)\n",
    "    str = (example.features.feature['1'].float_list.value[0])\n",
    "    print(getsizeof(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
