{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import clip_grad_norm_\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom tqdm.notebook import tqdm\nfrom torchvision.transforms.functional import InterpolationMode\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-16T22:15:49.580163Z","iopub.execute_input":"2022-08-16T22:15:49.580594Z","iopub.status.idle":"2022-08-16T22:15:49.587166Z","shell.execute_reply.started":"2022-08-16T22:15:49.580559Z","shell.execute_reply":"2022-08-16T22:15:49.586307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 142\n\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:08.807602Z","iopub.execute_input":"2022-08-16T19:41:08.808111Z","iopub.status.idle":"2022-08-16T19:41:08.816323Z","shell.execute_reply.started":"2022-08-16T19:41:08.808078Z","shell.execute_reply":"2022-08-16T19:41:08.815473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, data_dir, transforms=None):\n        monet_dir = os.path.join(data_dir, 'monet_jpg')\n        photo_dir = os.path.join(data_dir, 'photo_jpg')\n        \n        self.files_monet = [os.path.join(monet_dir, name) for name in sorted(os.listdir(monet_dir))]\n        self.files_photo = [os.path.join(photo_dir, name) for name in sorted(os.listdir(photo_dir))]\n        \n        self.transforms = transforms\n        \n    def __len__(self):\n        # we know that len(files_monet) = 300 < 7038 = len(files_photo)\n        return len(self.files_monet)\n    \n    def __getitem__(self, index):\n        # we will use only 300 (=len(files_monet)) photos during training\n        # randomly picking them from the first 300 photos\n        random_index = np.random.randint(0, len(self.files_monet))\n        file_monet = self.files_monet[index]\n        file_photo = self.files_photo[random_index]\n        \n        image_monet = Image.open(file_monet)\n        image_photo = Image.open(file_photo)\n        \n        if self.transforms is not None:\n            image_monet = self.transforms(image_monet)\n            image_photo = self.transforms(image_photo)\n        \n        return image_monet, image_photo","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:08.817831Z","iopub.execute_input":"2022-08-16T19:41:08.818355Z","iopub.status.idle":"2022-08-16T19:41:08.829867Z","shell.execute_reply.started":"2022-08-16T19:41:08.818313Z","shell.execute_reply":"2022-08-16T19:41:08.829066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/gan-getting-started'\nbatch_size = 5","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:08.831601Z","iopub.execute_input":"2022-08-16T19:41:08.832351Z","iopub.status.idle":"2022-08-16T19:41:08.841660Z","shell.execute_reply.started":"2022-08-16T19:41:08.832318Z","shell.execute_reply":"2022-08-16T19:41:08.840856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms_ = transforms.Compose([\n    #transforms.Resize((256, 256)), # photos already have the same size\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n])\n\ndataloader = DataLoader(\n    ImageDataset(data_dir, transforms=transforms_),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=2,\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:08.843243Z","iopub.execute_input":"2022-08-16T19:41:08.843864Z","iopub.status.idle":"2022-08-16T19:41:09.366879Z","shell.execute_reply.started":"2022-08-16T19:41:08.843831Z","shell.execute_reply":"2022-08-16T19:41:09.365849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unnorm(img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n    for t, m, s in zip(img, mean, std):\n        t.mul_(s).add_(s)\n        \n    return img","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.368721Z","iopub.execute_input":"2022-08-16T19:41:09.369456Z","iopub.status.idle":"2022-08-16T19:41:09.375843Z","shell.execute_reply.started":"2022-08-16T19:41:09.369411Z","shell.execute_reply":"2022-08-16T19:41:09.374973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build ASA Style transfer","metadata":{}},{"cell_type":"markdown","source":"## Auxiliary blocks","metadata":{}},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,\n                 transpose=False, use_leaky=True, use_dropout=False, normalize=True):\n        \n        super(ConvBlock, self).__init__()\n        self.block = []\n        \n        if transpose:\n            self.block += [nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n                                              stride, padding, output_padding=1)]\n        else:\n            self.block += [nn.Conv2d(in_channels, out_channels, kernel_size,\n                                     stride, padding, bias=True)]\n            \n        if normalize:\n            self.block += [nn.InstanceNorm2d(out_channels,affine=True)]\n            \n        if use_dropout:\n            self.block += [nn.Dropout(0.5)]\n            \n        if use_leaky:\n            self.block += [nn.LeakyReLU(negative_slope=0.2, inplace=True)]\n        else:\n            self.block += [nn.ReLU(inplace=True)]\n            \n        self.block = nn.Sequential(*self.block)\n    \n    \n    def forward(self, x):\n        return self.block(x)\n    \n    \nclass ResidualBlock(nn.Module):\n    def __init__(self, channels,kernel_size=3):\n        super(ResidualBlock, self).__init__()\n        \n        #doesn't change shape of input\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(int((kernel_size - 1) / 2)),\n            ConvBlock(in_channels=channels, out_channels=channels,padding=0,\n                      kernel_size=kernel_size, use_leaky=False, use_dropout=True),\n            nn.ReflectionPad2d(int((kernel_size - 1) / 2)),\n            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size,padding=0),\n            nn.InstanceNorm2d(channels,affine=True)\n        )\n    \n    \n    def forward(self, x):\n        return x + self.block(x)\n    \n    \nclass transformer_(nn.Module):\n    \n    def __init__(self,transformer_kernel_size=9):\n        super(transformer_, self).__init__()\n        #self.model=nn.AvgPool2d(transformer_kernel_size, stride=1,padding=transformer_kernel_size//2)\n        self.model=nn.Conv2d(in_channels=3, out_channels=3, kernel_size=transformer_kernel_size, stride=1,padding=transformer_kernel_size//2)\n\n    def forward(self,x):\n        return self.model(x)\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.377726Z","iopub.execute_input":"2022-08-16T19:41:09.378485Z","iopub.status.idle":"2022-08-16T19:41:09.396802Z","shell.execute_reply.started":"2022-08-16T19:41:09.378442Z","shell.execute_reply":"2022-08-16T19:41:09.396099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Discriminator","metadata":{}},{"cell_type":"code","source":"class discriminator_(nn.Module):\n    def __init__(self, in_channels=3,pad_type='valid',n_filter_discriminator=64):#ASA code use \"valid\" padding\n        super(discriminator_, self).__init__()\n        \n        #3*256*256 - 128*128*128 -> 1*128*128  \n        self.conv0=ConvBlock(in_channels=in_channels, out_channels=n_filter_discriminator*2, kernel_size=5,\n                      stride=2, padding=2, normalize=True)\n        self.conv0_pred=nn.Conv2d(in_channels=n_filter_discriminator*2, out_channels=1, kernel_size=5,\n                                     stride=1, padding=2, bias=True)\n        \n        #128*128*128 - 128*64*64 -> 1*65*65\n        self.conv1=ConvBlock(in_channels=n_filter_discriminator*2, out_channels=n_filter_discriminator*2, kernel_size=5,\n                      stride=2, padding=2, normalize=True)\n        self.conv1_pred=nn.Conv2d(in_channels=n_filter_discriminator*2, out_channels=1, kernel_size=10,\n                                     stride=1, padding=5, bias=True)\n        \n        #128*64*64 - 256*32*32\n        self.conv2=ConvBlock(in_channels=n_filter_discriminator*2, out_channels=n_filter_discriminator*4, kernel_size=5,\n                      stride=2, padding=2, normalize=True)\n        \n        #256*32*32 - 512*16*16 -> 1*17*17\n        self.conv3=ConvBlock(in_channels=n_filter_discriminator*4, out_channels=n_filter_discriminator*8, kernel_size=5,\n                      stride=2, padding=2, normalize=True)\n        self.conv3_pred=nn.Conv2d(in_channels=n_filter_discriminator*8, out_channels=1, kernel_size=10,\n                                     stride=1, padding=5, bias=True)\n        \n        #512*16*16 - 512*16*16 \n        self.conv4=ConvBlock(in_channels=n_filter_discriminator*8, out_channels=n_filter_discriminator*8, kernel_size=5,\n                      stride=1, padding=2, normalize=True)\n        self.conv4_pred=ConvBlock(in_channels=n_filter_discriminator*8, out_channels=1, kernel_size=5,\n                      stride=1, padding=2, normalize=True)\n        \n        #512*4*4 - 1024*4*4 -> 1*5*5\n        self.conv5=ConvBlock(in_channels=n_filter_discriminator*8, out_channels=n_filter_discriminator*16, kernel_size=5,\n                      stride=1, padding=2, normalize=True)\n        self.conv5_pred=nn.Conv2d(in_channels=n_filter_discriminator*16, out_channels=1, kernel_size=6,\n                                     stride=1, padding=3, bias=True)\n        \n        #1024*1*1 - 1024*2*2 -> 1*2*2\n        #self.conv6=ConvBlock(in_channels=n_filter_discriminator*16, out_channels=n_filter_discriminator*16, kernel_size=1,\n        #              stride=2, padding=0, normalize=True)\n        #self.conv6_pred=nn.Conv2d(in_channels=n_filter_discriminator*16, out_channels=1, kernel_size=3,\n        #                             stride=1, padding=1, bias=True)\n        \n        \n        \n    def forward(self, x):  \n        \n        h0=self.conv0(x)\n        h0_pred=self.conv0_pred(h0)\n        \n        h1=self.conv1(h0)\n        h1_pred=self.conv1_pred(h1)\n        \n        h2=self.conv2(h1)\n        \n        h3=self.conv3(h2)\n        h3_pred=self.conv3_pred(h3)\n        \n        h4=self.conv4(h3)\n        h4_pred=self.conv4_pred(h4)\n\n        h5=self.conv5(h4)\n        h5_pred=self.conv5_pred(h5)\n        \n        #h6=self.conv6(h5)\n        #h6_pred=self.conv6_pred(h6)\n        \n        #print((h0.size(),h1.size(),h2.size(),h3.size(),h4.size(),h5.size(),h6.size()))\n        #print((h0_pred.size(),h1_pred.size(),h3_pred.size(),h5_pred.size(),h6_pred.size()))\n        #return [h0_pred, h1_pred, h3_pred, h5_pred]#, h6_pred]\n        return [h4_pred]","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.400134Z","iopub.execute_input":"2022-08-16T19:41:09.401750Z","iopub.status.idle":"2022-08-16T19:41:09.417778Z","shell.execute_reply.started":"2022-08-16T19:41:09.401700Z","shell.execute_reply":"2022-08-16T19:41:09.416895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels):\n        super(Discriminator, self).__init__()\n        \n        self.model = nn.Sequential(\n            # 3*256*256 -> 64*128*128 \n            ConvBlock(in_channels=in_channels, out_channels=64, kernel_size=4,\n                      stride=2, padding=1, normalize=False),\n            \n            # 64*128*128 -> 128*64*64\n            ConvBlock(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n            \n            # 128*64*64 -> 256*32*32\n            ConvBlock(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n            \n            # 256*32*32 -> 512*31*31\n            ConvBlock(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1),\n            \n            # 512*31*31 -> 1*30*30\n            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1),\n        )\n        \n        \n    def forward(self, x):    \n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.418873Z","iopub.execute_input":"2022-08-16T19:41:09.419542Z","iopub.status.idle":"2022-08-16T19:41:09.436284Z","shell.execute_reply.started":"2022-08-16T19:41:09.419508Z","shell.execute_reply":"2022-08-16T19:41:09.435314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Generator","metadata":{}},{"cell_type":"code","source":"class encoder_(nn.Module):\n    \n    def __init__(self,in_channels=3,n_filter_generator=32,pad_type=0):\n        super(encoder_, self).__init__()\n        \n        self.model=nn.Sequential(\n            \n            nn.InstanceNorm2d(in_channels,affine=True),\n            \n            #3*256*256 - 3*262*262\n            nn.ReflectionPad2d(3),\n            \n            #3*262*262 - 64*259*259\n            ConvBlock(in_channels=in_channels, out_channels=n_filter_generator*2, kernel_size=4,\n                      stride=1, padding=pad_type, normalize=True,use_leaky=False),\n            #64*259*259 - 128*129*129\n            ConvBlock(in_channels=n_filter_generator*2, out_channels=4*n_filter_generator, kernel_size=4,\n                      stride=2, padding=1, normalize=True,use_leaky=False),\n            #128*129*129 - 256*64*64\n            ConvBlock(in_channels=4*n_filter_generator, out_channels=n_filter_generator*8, kernel_size=4,\n                      stride=2, padding=1, normalize=True,use_leaky=False),\n            #128*70*70 - 256*64*64\n            #ConvBlock(in_channels=n_filter_generator*4, out_channels=n_filter_generator*8, kernel_size=5,\n            #          stride=1, padding=pad_type, normalize=True,use_leaky=False),\n            #256*66*34 - 256*66*66\n            #ConvBlock(in_channels=n_filter_generator*8, out_channels=n_filter_generator*8, kernel_size=5,\n            #          stride=1, padding=pad_type, normalize=True,use_leaky=False)\n        )\n        \n    def forward(self, x):\n        return self.model(x)\n    \nclass decoder_(nn.Module):\n    \n    \"\"\"decoder model following https://arxiv.org/pdf/1807.10201.pdf\n    Returns: decoder model\n    \"\"\"\n    def __init__(self, input_shape,n_filter_generator=32):\n        super(decoder_, self).__init__()\n        \n        num_kernels = n_filter_generator * 8\n        self.model=[]\n        for i in range(9):\n            self.model.append(ResidualBlock(num_kernels))\n        \n        \n        #i=0: 256*64*64 - 256*64*64 - 128*64*64\n        #i=1: 128*64*64 - 128*128*128 - 64*128*128\n        #i=2: 64*128*128 - 64*256*256 - 32*256*256\n        ####i=3: 64*128*128 - 64*256*256 - 32*256*256\n        in_channels=[num_kernels,4*n_filter_generator,2*n_filter_generator,n_filter_generator]\n        for i in range(3):\n            #self.model.append(nn.ConvTranspose2d(in_channels=in_channels[i], out_channels=n_filter_generator * 2 ** (2 - i), kernel_size=5,\n            #                                  stride=2, padding=2, output_padding=1))\n            self.model.append(transforms.Resize(size=(32*2**(i+1),32*2**(i+1)), interpolation=InterpolationMode.NEAREST, max_size=None, antialias=None))\n            self.model.append(nn.Conv2d(in_channels=in_channels[i], out_channels=n_filter_generator * 2 ** (2 - i), kernel_size=3,\n                                     stride=1, padding=1, bias=True))\n            self.model.append(nn.InstanceNorm2d(n_filter_generator * 2 ** (2 - i),affine=True))\n            self.model.append(nn.LeakyReLU(inplace=True))\n            \n        self.model.append(nn.ReflectionPad2d(3))\n        \n        #32*256*256 - 3*256*256\n        self.model.append(nn.Conv2d(in_channels=n_filter_generator, out_channels=3, kernel_size=7,\n                                     stride=1, padding=0, bias=True))\n        self.model.append(nn.Tanh())\n        self.model = nn.Sequential(*self.model)\n        #x = CenterLayer()(x)\n\n    def forward(self,x):\n        return self.model(x)#*2-1","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.438819Z","iopub.execute_input":"2022-08-16T19:41:09.439288Z","iopub.status.idle":"2022-08-16T19:41:09.458055Z","shell.execute_reply.started":"2022-08-16T19:41:09.439257Z","shell.execute_reply":"2022-08-16T19:41:09.456871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, in_channels, out_channels, num_residual_blocks=9):\n        super(Generator, self).__init__()\n        \n        ''' Encoder '''\n        # Inital layer:  3*256*256 -> 64*256*256\n        self.initial = [\n            nn.ReflectionPad2d(in_channels),\n            ConvBlock(in_channels=in_channels, out_channels=64,\n                      kernel_size=2*in_channels+1, use_leaky=False),\n        ]\n        self.initial = nn.Sequential(*self.initial)\n        \n        # Downsampling:  64*256*256 -> 128*128*128 -> 256*64*64\n        self.down = [\n            ConvBlock(in_channels=64, out_channels=128, kernel_size=3,\n                      stride=2, padding=1, use_leaky=False),\n            ConvBlock(in_channels=128, out_channels=256, kernel_size=3,\n                      stride=2, padding=1, use_leaky=False),\n        ]\n        self.down = nn.Sequential(*self.down)\n        \n        \n        \"\"\" Transformer \"\"\"\n        # ResNet:  256*64*64 -> 256*64*64\n        self.transform = [ResidualBlock(256) for _ in range(num_residual_blocks)]\n        self.transform = nn.Sequential(*self.transform)\n        \n        \n        \"\"\" Decoder \"\"\"\n        # Upsampling:  256*64*64 -> 128*128*128 -> 64*256*256\n        self.up = [\n            ConvBlock(in_channels=256, out_channels=128, kernel_size=3, stride=2,\n                      padding=1, transpose=True, use_leaky=False),\n            ConvBlock(in_channels=128, out_channels=64, kernel_size=3, stride=2,\n                      padding=1, transpose=True, use_leaky=False),\n        ]\n        self.up = nn.Sequential(*self.up)\n        \n        # Out layer:  64*256*256 -> 3*256*256\n        self.out = nn.Sequential(\n            nn.ReflectionPad2d(out_channels),\n            nn.Conv2d(in_channels=64, out_channels=out_channels, kernel_size=2*out_channels+1),\n            nn.Tanh()\n        )\n    \n    \n    def forward(self, x):\n        x = self.down(self.initial(x))\n        x = self.transform(x)\n        x = self.out(self.up(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.459667Z","iopub.execute_input":"2022-08-16T19:41:09.460003Z","iopub.status.idle":"2022-08-16T19:41:09.489244Z","shell.execute_reply.started":"2022-08-16T19:41:09.459973Z","shell.execute_reply":"2022-08-16T19:41:09.488447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Losses","metadata":{}},{"cell_type":"code","source":"criterion_GAN = nn.MSELoss()\ncriterion_cycle = nn.L1Loss()\ncriterion_identity = nn.L1Loss()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.490639Z","iopub.execute_input":"2022-08-16T19:41:09.491143Z","iopub.status.idle":"2022-08-16T19:41:09.505838Z","shell.execute_reply.started":"2022-08-16T19:41:09.491110Z","shell.execute_reply":"2022-08-16T19:41:09.504891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _discriminator_loss(logits, labels):\n    #return torch.mean(F.binary_cross_entropy_with_logits(logits, labels, reduction='none'))\n    return criterion_GAN(logits, labels)\ndef discriminator_loss(discriminate_encoded_picture=None, discriminate_picture=None, discriminate_art=None):\n    \"\"\"\n    When training the discriminator, it needs to lean to classify the original art as 1 and the rest as 0\n    Args:\n        discriminate_encoded_picture:\n        discriminate_picture:\n        discriminate_art:\n    Returns:\n    \"\"\"\n    art_loss = 0\n    picture_loss = 0\n    encoded_picture_loss = 0\n    if discriminate_art!=None:\n        for pred in discriminate_art:\n            art_loss += _discriminator_loss(pred, torch.ones_like(pred).to(device))\n            #print(('art',_discriminator_loss(pred, torch.ones_like(pred))))\n    if discriminate_picture!=None:\n        for pred in discriminate_picture:\n            picture_loss += _discriminator_loss(pred, torch.zeros_like(pred).to(device))\n\n    if discriminate_encoded_picture!=None:\n        for pred in discriminate_encoded_picture:\n            encoded_picture_loss += _discriminator_loss(pred, torch.zeros_like(pred).to(device))\n          \n    global_loss = (art_loss + picture_loss + encoded_picture_loss)\n\n    return global_loss\n\ndef discriminator_acc(discriminate_encoded_picture, discriminate_picture, discriminate_art):\n    art = []\n    picture = []\n    encoded = []\n    for pred in discriminate_art:\n        art.append(torch.mean((pred > torch.zeros_like(pred)).float()))\n    for pred in discriminate_picture:\n        picture.append(torch.mean((pred < torch.zeros_like(pred)).float()))\n    for pred in discriminate_encoded_picture:\n        encoded.append(torch.mean((pred < torch.zeros_like(pred)).float()))\n    global_accuracy = torch.stack(art + picture + encoded, dim=0).sum(dim=0)\n\n    return global_accuracy","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.507464Z","iopub.execute_input":"2022-08-16T19:41:09.508564Z","iopub.status.idle":"2022-08-16T19:41:09.525797Z","shell.execute_reply.started":"2022-08-16T19:41:09.508519Z","shell.execute_reply":"2022-08-16T19:41:09.524594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def abs_criterion(logits, target):\n    \"\"\"absolute criterion or L1 norm\n    \"\"\"\n    return torch.mean(torch.abs(logits - target))\n\n\ndef mse_criterion(logits, target):\n    return torch.mean((logits - target) ** 2)\n\ndef generator_loss(disc_output=None, transformed_input_image=None, input_features=None, transformed_output_image=None, output_features=None, img_loss_weight=1., feature_loss_weight=1.,\n                   generator_weight=1.):\n    losses = []\n    if disc_output!=None:\n        for pred in disc_output:\n            losses.append(_discriminator_loss(pred, torch.ones_like(pred).to(device)))#should be zero like?\n\n    generator_global_loss = torch.stack(losses, dim=0).sum(dim=0) * generator_weight\n\n    # Image loss.\n    img_loss=0\n    if transformed_output_image!=None:\n        img_loss = mse_criterion(transformed_output_image, transformed_input_image) * img_loss_weight\n\n    # Features loss.\n    feature_loss=0\n    if input_features!=None:\n        feature_loss = abs_criterion(output_features, input_features) * feature_loss_weight\n\n    global_loss = (img_loss + feature_loss + generator_global_loss)\n    #global_loss = generator_global_loss# (feature_loss)# + generator_global_loss)\n\n    return global_loss\n\ndef generator_acc(disc_output):\n    accuracies = []\n    for pred in disc_output:\n        accuracies.append(torch.mean((pred > torch.zeros_like(pred)).float()))\n\n    generator_global_accuracy = torch.stack(accuracies, dim=0).sum(dim=0)\n\n    return generator_global_accuracy","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.527841Z","iopub.execute_input":"2022-08-16T19:41:09.528331Z","iopub.status.idle":"2022-08-16T19:41:09.539780Z","shell.execute_reply.started":"2022-08-16T19:41:09.528299Z","shell.execute_reply":"2022-08-16T19:41:09.538871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Initialization","metadata":{}},{"cell_type":"code","source":"#encoder=encoder_().to(device)\n#decoder=decoder_(input_shape=(batch_size,256,16,16)).to(device)\n#discriminator = discriminator_().to(device)\n#transformer = transformer_().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.541276Z","iopub.execute_input":"2022-08-16T19:41:09.542283Z","iopub.status.idle":"2022-08-16T19:41:09.554165Z","shell.execute_reply.started":"2022-08-16T19:41:09.542240Z","shell.execute_reply":"2022-08-16T19:41:09.553217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ngenerator_monet2photo = nn.Sequential(encoder_(),decoder_(input_shape=(batch_size,256,64,64))).to(device)\ngenerator_photo2monet = nn.Sequential(encoder_(),decoder_(input_shape=(batch_size,256,64,64))).to(device)\n\ndiscriminator_monet = Discriminator(in_channels=3).to(device)\ndiscriminator_photo = Discriminator(in_channels=3).to(device)\n'''","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.555916Z","iopub.execute_input":"2022-08-16T19:41:09.556628Z","iopub.status.idle":"2022-08-16T19:41:09.571193Z","shell.execute_reply.started":"2022-08-16T19:41:09.556584Z","shell.execute_reply":"2022-08-16T19:41:09.570029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_m2p=encoder_().to(device)\nencoder_p2m=encoder_().to(device)\ndecoder_m2p=decoder_(input_shape=(batch_size,256,64,64)).to(device)\ndecoder_p2m=decoder_(input_shape=(batch_size,256,64,64)).to(device)\n\n#generator_monet2photo = nn.Sequential(encoder1,decoder1).to(device)\n#generator_photo2monet = nn.Sequential(encoder2,decoder2).to(device)\n\ndiscriminator_monet = Discriminator(in_channels=3).to(device)\ndiscriminator_photo = Discriminator(in_channels=3).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.573165Z","iopub.execute_input":"2022-08-16T19:41:09.574089Z","iopub.status.idle":"2022-08-16T19:41:09.915387Z","shell.execute_reply.started":"2022-08-16T19:41:09.574042Z","shell.execute_reply":"2022-08-16T19:41:09.914389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimization Setup","metadata":{}},{"cell_type":"code","source":"'''\nlr1=2e-5\nlr2 = 2e-5\nb1 = 0.9\nb2 = 0.999\n\noptim_generators = torch.optim.Adam(\n    list(encoder.parameters()) + list(transformer.parameters()) + list(decoder.parameters()),\n    lr=lr1, betas=(b1, b2)\n)\n\noptim_discriminators = torch.optim.Adam(\n    list(discriminator.parameters()),\n    lr=lr2, betas=(b1, b2)\n)\n'''\n","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.916616Z","iopub.execute_input":"2022-08-16T19:41:09.917083Z","iopub.status.idle":"2022-08-16T19:41:09.923436Z","shell.execute_reply.started":"2022-08-16T19:41:09.917038Z","shell.execute_reply":"2022-08-16T19:41:09.922462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nnum_epochs = 80\ndecay_epoch = 25\n\nlr_sched_step = lambda epoch: 1 - max(0, epoch - decay_epoch) / (num_epochs - decay_epoch)\n\nlr_sched_generators = torch.optim.lr_scheduler.LambdaLR(optim_generators, lr_lambda=lr_sched_step)\nlr_sched_discriminators = torch.optim.lr_scheduler.LambdaLR(optim_discriminators, lr_lambda=lr_sched_step)\n'''\n","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.925266Z","iopub.execute_input":"2022-08-16T19:41:09.925795Z","iopub.status.idle":"2022-08-16T19:41:09.935407Z","shell.execute_reply.started":"2022-08-16T19:41:09.925683Z","shell.execute_reply":"2022-08-16T19:41:09.934500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nlr = 2e-4\nb1 = 0.5\nb2 = 0.999\n\noptim_generators = torch.optim.Adam(\n    list(generator_monet2photo.parameters()) + list(generator_photo2monet.parameters()),\n    lr=lr, betas=(b1, b2)\n)\n\noptim_discriminators = torch.optim.Adam(\n    list(discriminator_monet.parameters()) + list(discriminator_photo.parameters()),\n    lr=lr, betas=(b1, b2)\n)\n'''\n","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.936628Z","iopub.execute_input":"2022-08-16T19:41:09.937184Z","iopub.status.idle":"2022-08-16T19:41:09.949585Z","shell.execute_reply.started":"2022-08-16T19:41:09.937152Z","shell.execute_reply":"2022-08-16T19:41:09.948707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nnum_epochs = 80\ndecay_epoch = 25\n\nlr_sched_step = lambda epoch: 1 - max(0, epoch - decay_epoch) / (num_epochs - decay_epoch)\n\nlr_sched_generators = torch.optim.lr_scheduler.LambdaLR(optim_generators, lr_lambda=lr_sched_step)\nlr_sched_discriminators = torch.optim.lr_scheduler.LambdaLR(optim_discriminators, lr_lambda=lr_sched_step)\n'''\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.950892Z","iopub.execute_input":"2022-08-16T19:41:09.951218Z","iopub.status.idle":"2022-08-16T19:41:09.966074Z","shell.execute_reply.started":"2022-08-16T19:41:09.951188Z","shell.execute_reply":"2022-08-16T19:41:09.965387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 2e-4\nb1 = 0.5\nb2 = 0.999\n\noptim_generators = torch.optim.Adam(\n    list(encoder_m2p.parameters()) + list(encoder_p2m.parameters())\n     + list(decoder_m2p.parameters()) + list(decoder_p2m.parameters()),\n    lr=lr, betas=(b1, b2)\n)\n\noptim_discriminators = torch.optim.Adam(\n    list(discriminator_monet.parameters()) + list(discriminator_photo.parameters()),\n    lr=lr, betas=(b1, b2)\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.967266Z","iopub.execute_input":"2022-08-16T19:41:09.967670Z","iopub.status.idle":"2022-08-16T19:41:09.979869Z","shell.execute_reply.started":"2022-08-16T19:41:09.967639Z","shell.execute_reply":"2022-08-16T19:41:09.978817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 60\ndecay_epoch = 25\n\nlr_sched_step = lambda epoch: 1 - max(0, epoch - decay_epoch) / (num_epochs - decay_epoch)\n\nlr_sched_generators = torch.optim.lr_scheduler.LambdaLR(optim_generators, lr_lambda=lr_sched_step)\nlr_sched_discriminators = torch.optim.lr_scheduler.LambdaLR(optim_discriminators, lr_lambda=lr_sched_step)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.981148Z","iopub.execute_input":"2022-08-16T19:41:09.981835Z","iopub.status.idle":"2022-08-16T19:41:09.997644Z","shell.execute_reply.started":"2022-08-16T19:41:09.981799Z","shell.execute_reply":"2022-08-16T19:41:09.996586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Learning rate schedulers, auxiliary tools, ","metadata":{}},{"cell_type":"code","source":"class History:\n    def __init__(self):\n        self.generators_loss = []\n        self.discriminators_loss = []\n    \n    def update(self, gen_loss, discr_loss):\n        self.generators_loss.append(gen_loss)\n        self.discriminators_loss.append(discr_loss)\n        \n    def show(self, title='Losses'):\n        fig = plt.figure(figsize=(20, 8))\n        plt.title(title)\n        plt.plot(self.generators_loss, 'o-', color='r',\n                 linewidth=2, markersize=3, label='Generators Loss')\n        plt.plot(self.discriminators_loss, 'o-', color='b',\n                 linewidth=2, markersize=3, label='Discriminators Loss')\n        plt.legend(loc='best')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.grid(True)\n        plt.show()\n        \nclass Buffer:\n    def __init__(self, max_images=50):\n        self.max_images = max_images\n        self.images = []\n        \n    def update(self, images):\n        images = images.detach().cpu().data.numpy()\n        for image in images:\n            if len(self.images) < self.max_images:\n                self.images.append(image)\n            else:\n                if np.random.rand() > 0.5:\n                    index = np.random.randint(0, self.max_images)\n                    self.images[index] = image\n\n    def sample(self, num_images):\n        samples = np.array([self.images[np.random.randint(0, len(self.images))]\n                            for _ in range(num_images)])\n        return torch.tensor(samples)\ndef update_req_grad(models, requires_grad=True):\n    for model in models:\n        for param in model.parameters():\n            param.requires_grad = requires_grad","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:09.998790Z","iopub.execute_input":"2022-08-16T19:41:09.999216Z","iopub.status.idle":"2022-08-16T19:41:10.012155Z","shell.execute_reply.started":"2022-08-16T19:41:09.999187Z","shell.execute_reply":"2022-08-16T19:41:10.011457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"\nhistory = History()\nbuffer_monet = Buffer()\nbuffer_photo = Buffer()\n\nfor epoch in range(num_epochs):\n    avg_generators_loss = 0\n    avg_discriminators_loss = 0\n    \n    for i, (real_monet, real_photo) in enumerate(tqdm(dataloader, leave=False, total=len(dataloader))):\n        real_monet, real_photo = real_monet.to(device), real_photo.to(device)\n                \n        \"\"\" Train Generators \"\"\"\n        # switching models parameters so that only generators are trained\n        #update_req_grad([generator_monet2photo, generator_photo2monet], True)\n        update_req_grad([encoder_m2p, encoder_p2m, decoder_m2p, decoder_p2m], True)\n        update_req_grad([discriminator_monet, discriminator_photo], False)\n        \n        # zero the parameters gradients\n        optim_generators.zero_grad()\n        \n        # forward-pass\n        encoded_monet=encoder_m2p(real_monet)\n        encoded_photo=encoder_p2m(real_photo)\n\n        fake_photo = decoder_m2p(encoded_monet)\n        fake_monet = decoder_p2m(encoded_photo)\n        \n        latent_photo=encoder_p2m(fake_photo)\n        latent_monet=encoder_m2p(fake_monet)\n\n        cycle_photo = decoder_m2p(encoder_m2p(fake_monet))\n        cycle_monet = decoder_p2m(encoder_p2m(fake_photo))\n\n        identity_photo = decoder_m2p(encoder_m2p(real_photo))\n        identity_monet = decoder_p2m(encoder_p2m(real_monet))\n        \n        #cycle_photo = generator_monet2photo(fake_monet)\n        #cycle_monet = generator_photo2monet(fake_photo)\n        \n        #identity_photo = generator_monet2photo(real_photo)\n        #identity_monet = generator_photo2monet(real_monet)\n        \n        # update photos that are used to feed up discriminators\n        buffer_photo.update(fake_photo)\n        buffer_monet.update(fake_monet)\n        \n        # discriminators outputs that are used in adversarial loss\n        discriminator_outputs_photo = discriminator_photo(fake_photo)\n        discriminator_outputs_monet = discriminator_monet(fake_monet)\n        \n        # labels that are used as ground truth\n        labels_real = torch.ones(discriminator_outputs_monet.size()).to(device)\n        labels_fake = torch.zeros(discriminator_outputs_monet.size()).to(device)\n        \n        # adversarial loss - enforces that the generated output be of the appropriate domain\n        loss_GAN_monet2photo = criterion_GAN(discriminator_outputs_photo, labels_real)\n        loss_GAN_photo2monet = criterion_GAN(discriminator_outputs_monet, labels_real)\n        loss_GAN = (loss_GAN_monet2photo + loss_GAN_photo2monet) / 2\n        #loss_GAN=generator_loss(disc_output=discriminator_outputs_photo)+generator_loss(disc_output=discriminator_outputs_monet)\n        \n        # cycle consistency loss - enforces that the input and output are recognizably the same\n        loss_cycle_photo = criterion_cycle(cycle_photo, real_photo)\n        loss_cycle_monet = criterion_cycle(cycle_monet, real_monet)\n        loss_cycle = (loss_cycle_photo + loss_cycle_monet) / 2\n        \n        # identity mapping loss - helps preserve the color of the input images\n        loss_identity_photo = criterion_identity(identity_photo, real_photo)\n        loss_identity_monet = criterion_identity(identity_monet, real_monet)\n        loss_identity = (loss_identity_photo + loss_identity_monet) / 2\n        \n        loss_latent_photo=criterion_cycle(latent_photo, encoded_monet)\n        loss_latent_monet=criterion_cycle(latent_monet, encoded_photo)\n\n        # total loss\n        #print(0.1*loss_latent_photo)\n        #print(10*loss_cycle)\n        #print(5* loss_identity)\n        loss_generators_total = loss_GAN + 0.2*loss_latent_photo + 0.2*loss_latent_monet + 10 * loss_cycle + 5 * loss_identity\n        #loss_generators_total = loss_GAN + 10 * loss_cycle + 5 * loss_identity\n        \n        # backward-pass\n        loss_generators_total.backward()\n        optim_generators.step()\n        \n        # limiting gradient norms - if they exceed 100, something went wrong\n        #clip_grad_norm_(generator_photo2monet.parameters(), 100)\n        #clip_grad_norm_(generator_monet2photo.parameters(), 100)\n        clip_grad_norm_(encoder_m2p.parameters(), 100)\n        clip_grad_norm_(encoder_p2m.parameters(), 100)\n        clip_grad_norm_(decoder_m2p.parameters(), 100)\n        clip_grad_norm_(decoder_p2m.parameters(), 100)\n        \n        \n        \"\"\" Train Discriminators \"\"\"\n        # switching models parameters so that only discriminators alu trained\n        update_req_grad([discriminator_monet, discriminator_photo], True)\n        #update_req_grad([generator_monet2photo, generator_photo2monet], False)\n        update_req_grad([encoder_m2p, encoder_p2m, decoder_m2p, decoder_p2m], False)\n\n        # zero the parameters gradients\n        optim_discriminators.zero_grad()\n        \n        # sample images from 50 stored\n        fake_photo = buffer_photo.sample(num_images=batch_size).to(device)\n        fake_monet = buffer_monet.sample(num_images=batch_size).to(device)\n        \n        # making labels noisy for discriminators so that they don't prevail over generators\n        threshold = min(1, 0.85 + (1 - 0.85) * epoch / (num_epochs // 2))\n        noisy_labels_real = (torch.rand(discriminator_outputs_monet.size()) < threshold).float().to(device)\n        \n        # forward-pass + losses\n        loss_real_photo = criterion_GAN(discriminator_photo(real_photo), noisy_labels_real)\n        loss_fake_photo = criterion_GAN(discriminator_photo(fake_photo.detach()), labels_fake)\n        loss_photo = (loss_real_photo + loss_fake_photo) / 2\n        #loss_photo = discriminator_loss(discriminate_encoded_picture=discriminator_photo(fake_photo.detach()), discriminate_art=discriminator_photo(real_photo))\n\n        loss_real_monet = criterion_GAN(discriminator_monet(real_monet), noisy_labels_real)\n        loss_fake_monet = criterion_GAN(discriminator_monet(fake_monet.detach()), labels_fake)\n        loss_monet = (loss_real_monet + loss_fake_monet) / 2\n        #loss_monet = discriminator_loss(discriminate_encoded_picture=discriminator_monet(fake_monet.detach()), discriminate_art=discriminator_monet(real_monet))\n\n        loss_discriminators_total = loss_monet + loss_photo\n        \n        # backward-pass\n        loss_discriminators_total.backward()\n        optim_discriminators.step()\n        \n        # clipping gradients to avoid gradients explosion\n        clip_grad_norm_(discriminator_monet.parameters(), 100)\n        clip_grad_norm_(discriminator_photo.parameters(), 100)\n        \n        # updating intermediate results\n        avg_generators_loss += loss_generators_total.item()\n        avg_discriminators_loss += loss_discriminators_total.item()\n        \n    # saving intermediate results\n    avg_generators_loss /= len(dataloader)\n    avg_discriminators_loss /= len(dataloader)\n    history.update(avg_generators_loss, avg_discriminators_loss)\n    \n    # showing intermediate results\n    print(\"Epoch: %d/%d | Generators Loss: %.4f | Discriminators Loss: %.4f\"\n              % (epoch+1, num_epochs, avg_generators_loss, avg_discriminators_loss))\n    \n    # showing generated images\n    if (epoch + 1) % 5 == 0:\n        _, sample_real_photo = next(iter(dataloader))\n        \n        sample_fake_monet = decoder_p2m(encoder_p2m(sample_real_photo.to(device))).detach().cpu()\n        \n        num_photos = min(batch_size, 5)\n        plt.figure(figsize=(20, 8))\n        for k in range(num_photos):\n            plt.subplot(2, num_photos, k + 1)\n            plt.imshow(unnorm(sample_real_photo[k]).permute(1, 2, 0))\n            plt.title('Input photo')\n            plt.axis('off')\n\n            plt.subplot(2, num_photos, k + num_photos + 1)\n            plt.imshow(unnorm(sample_fake_monet[k]).permute(1, 2, 0))\n            plt.title('Output image')\n            plt.axis('off')\n        plt.show()\n    \n    lr_sched_generators.step()\n    lr_sched_discriminators.step()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:10.013651Z","iopub.execute_input":"2022-08-16T19:41:10.013974Z","iopub.status.idle":"2022-08-16T19:41:28.858750Z","shell.execute_reply.started":"2022-08-16T19:41:10.013946Z","shell.execute_reply":"2022-08-16T19:41:28.857124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nhistory = History()\nbuffer_monet = Buffer()\nbuffer_photo = Buffer()\n\nfor epoch in range(num_epochs):\n    avg_generators_loss = 0\n    avg_discriminators_loss = 0\n    \n    for i, (real_monet, real_photo) in enumerate(tqdm(dataloader, leave=False, total=len(dataloader))):\n        real_monet, real_photo = real_monet.to(device), real_photo.to(device)\n                \n        \"\"\" Train Generators \"\"\"\n        # switching models parameters so that only generators are trained\n        update_req_grad([generator_monet2photo, generator_photo2monet], True)\n        update_req_grad([discriminator_monet, discriminator_photo], False)\n        \n        # zero the parameters gradients\n        optim_generators.zero_grad()\n        \n        # forward-pass\n        fake_photo = generator_monet2photo(real_monet)\n        fake_monet = generator_photo2monet(real_photo)\n        \n        cycle_photo = generator_monet2photo(fake_monet)\n        cycle_monet = generator_photo2monet(fake_photo)\n        \n        identity_photo = generator_monet2photo(real_photo)\n        identity_monet = generator_photo2monet(real_monet)\n        \n        # update photos that are used to feed up discriminators\n        buffer_photo.update(fake_photo)\n        buffer_monet.update(fake_monet)\n        \n        # discriminators outputs that are used in adversarial loss\n        discriminator_outputs_photo = discriminator_photo(fake_photo)\n        discriminator_outputs_monet = discriminator_monet(fake_monet)\n        \n        # labels that are used as ground truth\n        labels_real = torch.ones(discriminator_outputs_monet.size()).to(device)\n        labels_fake = torch.zeros(discriminator_outputs_monet.size()).to(device)\n        \n        # adversarial loss - enforces that the generated output be of the appropriate domain\n        loss_GAN_monet2photo = criterion_GAN(discriminator_outputs_photo, labels_real)\n        loss_GAN_photo2monet = criterion_GAN(discriminator_outputs_monet, labels_real)\n        loss_GAN = (loss_GAN_monet2photo + loss_GAN_photo2monet) / 2\n        #loss_GAN=generator_loss(disc_output=discriminator_outputs_photo)+generator_loss(disc_output=discriminator_outputs_monet)\n        \n        # cycle consistency loss - enforces that the input and output are recognizably the same\n        loss_cycle_photo = criterion_cycle(cycle_photo, real_photo)\n        loss_cycle_monet = criterion_cycle(cycle_monet, real_monet)\n        loss_cycle = (loss_cycle_photo + loss_cycle_monet) / 2\n        \n        # identity mapping loss - helps preserve the color of the input images\n        loss_identity_photo = criterion_identity(identity_photo, real_photo)\n        loss_identity_monet = criterion_identity(identity_monet, real_monet)\n        loss_identity = (loss_identity_photo + loss_identity_monet) / 2\n        \n        # total loss\n        loss_generators_total = loss_GAN + 10 * loss_cycle + 5 * loss_identity\n        \n        # backward-pass\n        loss_generators_total.backward()\n        optim_generators.step()\n        \n        # limiting gradient norms - if they exceed 100, something went wrong\n        clip_grad_norm_(generator_photo2monet.parameters(), 100)\n        clip_grad_norm_(generator_monet2photo.parameters(), 100)\n        \n        \n        \"\"\" Train Discriminators \"\"\"\n        # switching models parameters so that only discriminators are trained\n        update_req_grad([discriminator_monet, discriminator_photo], True)\n        update_req_grad([generator_monet2photo, generator_photo2monet], False)\n        \n        # zero the parameters gradients\n        optim_discriminators.zero_grad()\n        \n        # sample images from 50 stored\n        fake_photo = buffer_photo.sample(num_images=batch_size).to(device)\n        fake_monet = buffer_monet.sample(num_images=batch_size).to(device)\n        \n        # making labels noisy for discriminators so that they don't prevail over generators\n        threshold = min(1, 0.85 + (1 - 0.85) * epoch / (num_epochs // 2))\n        noisy_labels_real = (torch.rand(discriminator_outputs_monet.size()) < threshold).float().to(device)\n        \n        # forward-pass + losses\n        loss_real_photo = criterion_GAN(discriminator_photo(real_photo), noisy_labels_real)\n        loss_fake_photo = criterion_GAN(discriminator_photo(fake_photo.detach()), labels_fake)\n        loss_photo = (loss_real_photo + loss_fake_photo) / 2\n        #loss_photo = discriminator_loss(discriminate_encoded_picture=discriminator_photo(fake_photo.detach()), discriminate_art=discriminator_photo(real_photo))\n\n        loss_real_monet = criterion_GAN(discriminator_monet(real_monet), noisy_labels_real)\n        loss_fake_monet = criterion_GAN(discriminator_monet(fake_monet.detach()), labels_fake)\n        loss_monet = (loss_real_monet + loss_fake_monet) / 2\n        #loss_monet = discriminator_loss(discriminate_encoded_picture=discriminator_monet(fake_monet.detach()), discriminate_art=discriminator_monet(real_monet))\n\n        loss_discriminators_total = loss_monet + loss_photo\n        \n        # backward-pass\n        loss_discriminators_total.backward()\n        optim_discriminators.step()\n        \n        # clipping gradients to avoid gradients explosion\n        clip_grad_norm_(discriminator_monet.parameters(), 100)\n        clip_grad_norm_(discriminator_photo.parameters(), 100)\n        \n        # updating intermediate results\n        avg_generators_loss += loss_generators_total.item()\n        avg_discriminators_loss += loss_discriminators_total.item()\n        \n    # saving intermediate results\n    avg_generators_loss /= len(dataloader)\n    avg_discriminators_loss /= len(dataloader)\n    history.update(avg_generators_loss, avg_discriminators_loss)\n    \n    # showing intermediate results\n    print(\"Epoch: %d/%d | Generators Loss: %.4f | Discriminators Loss: %.4f\"\n              % (epoch+1, num_epochs, avg_generators_loss, avg_discriminators_loss))\n    \n    # showing generated images\n    if (epoch + 1) % 10 == 0:\n        _, sample_real_photo = next(iter(dataloader))\n        \n        sample_fake_monet = generator_photo2monet(sample_real_photo.to(device)).detach().cpu()\n        \n        num_photos = min(batch_size, 5)\n        plt.figure(figsize=(20, 8))\n        for k in range(num_photos):\n            plt.subplot(2, num_photos, k + 1)\n            plt.imshow(unnorm(sample_real_photo[k]).permute(1, 2, 0))\n            plt.title('Input photo')\n            plt.axis('off')\n\n            plt.subplot(2, num_photos, k + num_photos + 1)\n            plt.imshow(unnorm(sample_fake_monet[k]).permute(1, 2, 0))\n            plt.title('Output image')\n            plt.axis('off')\n        plt.show()\n    \n    lr_sched_generators.step()\n    lr_sched_discriminators.step()\n    \n'''","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:28.860158Z","iopub.status.idle":"2022-08-16T19:41:28.860538Z","shell.execute_reply.started":"2022-08-16T19:41:28.860352Z","shell.execute_reply":"2022-08-16T19:41:28.860385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nhistory = History()\ndiscr_success_rate=0.8\n\nfor epoch in range(num_epochs):\n    avg_generators_loss = 0\n    avg_discriminators_loss = 0\n    \n    discr_success = 0\n    alpha = 0.05\n    train_generator_at_previous_step = True\n    for i, (real_monet, real_photo) in enumerate(tqdm(dataloader, leave=False, total=len(dataloader))):\n        real_monet, real_photo = real_monet.to(device), real_photo.to(device)\n        #print(real_monet)\n        #---------------------train generator-------------------------------------------\n        #if discr_success >= discr_success_rate:\n        #if train_generator_at_previous_step==False:\n        if True:\n            #print('1')\n            if not train_generator_at_previous_step:\n                train_generator_at_previous_step = True\n            \n            update_req_grad([discriminator], False)\n            update_req_grad([encoder, decoder, transformer], True)\n\n            # zero the parameters gradients\n            optim_generators.zero_grad()\n            \n            encoded_picture = encoder(real_photo)\n            decoded_picture = decoder(encoded_picture)\n            encoded_decoded_picture = encoder(decoded_picture)\n            discriminate_encoded_picture = discriminator(decoded_picture)\n            transformed_picture = transformer(real_photo)\n            transformed_decoded_picture = transformer(decoded_picture)\n            #print(('generator',encoded_picture,decoded_picture,encoded_decoded_picture,\n            #      discriminate_encoded_picture, transformed_picture, transformed_decoded_picture))\n            accuracy = generator_acc(discriminate_encoded_picture)\n            loss_generators_total = generator_loss(discriminate_encoded_picture, transformed_picture, encoded_picture, transformed_decoded_picture, encoded_decoded_picture)\n            print(loss_generators_total)\n            loss_generators_total.backward()\n            optim_generators.step()\n        \n            # limiting gradient norms - if they exceed 100, something went wrong\n            clip_grad_norm_(encoder.parameters(), 100)\n            clip_grad_norm_(decoder.parameters(), 100)\n            clip_grad_norm_(transformer.parameters(), 100)\n            \n            discr_success = discr_success * (1. - alpha) + alpha * (1 - accuracy)\n            lr_sched_generators.step()\n            # updating intermediate results\n            avg_generators_loss += loss_generators_total.item()\n        #----------------train discriminator--------------------------------------\n        else:\n            #print('2')\n            if train_generator_at_previous_step:\n                train_generator_at_previous_step = False\n                \n            update_req_grad([discriminator], True)\n            update_req_grad([encoder, decoder, transformer], False)\n\n            # zero the parameters gradients\n            optim_discriminators.zero_grad()\n            \n            encoded_picture = encoder(real_photo)\n            decoded_picture = decoder(encoded_picture)\n            \n            discriminate_encoded_picture = discriminator(decoded_picture.detach())\n            discriminate_picture = discriminator(real_photo.detach())\n            discriminate_art = discriminator(real_monet.detach())\n            \n            loss_discriminators_total = discriminator_loss(discriminate_encoded_picture, discriminate_picture, discriminate_art)\n            accuracy = discriminator_acc(discriminate_encoded_picture, discriminate_picture, discriminate_art)\n            \n            loss_discriminators_total.backward()\n            optim_discriminators.step()\n\n            # limiting gradient norms - if they exceed 100, something went wrong\n            clip_grad_norm_(discriminator.parameters(), 100)\n            discr_success = discr_success * (1. - alpha) + alpha * accuracy\n            #print(loss_discriminators_total.item())\n            lr_sched_discriminators.step()\n            # updating intermediate results\n            avg_discriminators_loss += loss_discriminators_total.item()\n\n    # saving intermediate results\n    avg_generators_loss /= len(dataloader)\n    avg_discriminators_loss /= len(dataloader)\n    history.update(avg_generators_loss, avg_discriminators_loss)\n    \n    # showing intermediate results\n    print(\"Epoch: %d/%d | Generators Loss: %.4f | Discriminators Loss: %.4f\"\n              % (epoch+1, num_epochs, avg_generators_loss, avg_discriminators_loss))\n    \n    # showing generated images\n    if (epoch + 1) % 10 == 0:\n        _, sample_real_photo = next(iter(dataloader))\n        \n        sample_fake_monet = decoder(encoder(sample_real_photo.to(device))).detach().cpu()\n        \n        num_photos = min(batch_size, 5)\n        plt.figure(figsize=(20, 8))\n        for k in range(num_photos):\n            plt.subplot(2, num_photos, k + 1)\n            plt.imshow(unnorm(sample_real_photo[k]).permute(1, 2, 0))\n            plt.title('Input photo')\n            plt.axis('off')\n\n            plt.subplot(2, num_photos, k + num_photos + 1)\n            plt.imshow(unnorm(sample_fake_monet[k]).permute(1, 2, 0))\n            plt.title('Output image')\n            plt.axis('off')\n        plt.show()\n    \n''' \n    ","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:28.864259Z","iopub.status.idle":"2022-08-16T19:41:28.864867Z","shell.execute_reply.started":"2022-08-16T19:41:28.864639Z","shell.execute_reply":"2022-08-16T19:41:28.864679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nhistory = History()\nbuffer_monet = Buffer()\nbuffer_photo = Buffer()\n\ntransformer = transformer_().to(device)\n\ngenerator_monet2photo = nn.Sequential(encoder_(),decoder_(input_shape=(batch_size,256,16,16))).to(device)\ngenerator_photo2monet = nn.Sequential(encoder_(),decoder_(input_shape=(batch_size,256,16,16))).to(device)\n\ndiscriminator_monet = Discriminator(in_channels=3).to(device)\ndiscriminator_photo = Discriminator(in_channels=3).to(device)\n\ncriterion_GAN = nn.MSELoss()\ncriterion_cycle = nn.L1Loss()\ncriterion_identity = nn.L1Loss()\n\nlr = 2e-4\nb1 = 0.5\nb2 = 0.999\n\noptim_generators = torch.optim.Adam(\n    list(generator_monet2photo.parameters()) + list(generator_photo2monet.parameters()),\n    lr=lr, betas=(b1, b2)\n)\n\noptim_discriminators = torch.optim.Adam(\n    list(discriminator_monet.parameters()) + list(discriminator_photo.parameters()),\n    lr=lr, betas=(b1, b2)\n)\n\nnum_epochs = 150\ndecay_epoch = 25\n\nlr_sched_step = lambda epoch: 1 - max(0, epoch - decay_epoch) / (num_epochs - decay_epoch)\n\nlr_sched_generators = torch.optim.lr_scheduler.LambdaLR(optim_generators, lr_lambda=lr_sched_step)\nlr_sched_discriminators = torch.optim.lr_scheduler.LambdaLR(optim_discriminators, lr_lambda=lr_sched_step)\nfor epoch in range(num_epochs):\n    avg_generators_loss = 0\n    avg_discriminators_loss = 0\n    \n    for i, (real_monet, real_photo) in enumerate(tqdm(dataloader, leave=False, total=len(dataloader))):\n        real_monet, real_photo = real_monet.to(device), real_photo.to(device)\n                \n        \"\"\" Train Generators \"\"\"\n        # switching models parameters so that only generators are trained\n        update_req_grad([generator_monet2photo, generator_photo2monet], True)\n        update_req_grad([discriminator_monet, discriminator_photo], False)\n        \n        # zero the parameters gradients\n        optim_generators.zero_grad()\n        \n        # forward-pass\n        fake_photo = generator_monet2photo(real_monet)\n        fake_monet = generator_photo2monet(real_photo)\n        \n        cycle_photo = generator_monet2photo(fake_monet)\n        cycle_monet = generator_photo2monet(fake_photo)\n        \n        identity_photo = generator_monet2photo(real_photo)\n        identity_monet = generator_photo2monet(real_monet)\n        \n        # update photos that are used to feed up discriminators\n        buffer_photo.update(fake_photo)\n        buffer_monet.update(fake_monet)\n        \n        # discriminators outputs that are used in adversarial loss\n        discriminator_outputs_photo = discriminator_photo(fake_photo)\n        discriminator_outputs_monet = discriminator_monet(fake_monet)\n        \n        # labels that are used as ground truth\n        labels_real = torch.ones(discriminator_outputs_monet.size()).to(device)\n        labels_fake = torch.zeros(discriminator_outputs_monet.size()).to(device)\n        \n        # adversarial loss - enforces that the generated output be of the appropriate domain\n        loss_GAN_monet2photo = criterion_GAN(discriminator_outputs_photo, labels_real)\n        loss_GAN_photo2monet = criterion_GAN(discriminator_outputs_monet, labels_real)\n        loss_GAN = (loss_GAN_monet2photo + loss_GAN_photo2monet) / 2\n        \n        # cycle consistency loss - enforces that the input and output are recognizably the same\n        loss_cycle_photo = criterion_cycle(cycle_photo, real_photo)\n        loss_cycle_monet = criterion_cycle(cycle_monet, real_monet)\n        loss_cycle = (loss_cycle_photo + loss_cycle_monet) / 2\n        \n        # identity mapping loss - helps preserve the color of the input images\n        loss_identity_photo = criterion_identity(identity_photo, real_photo)\n        loss_identity_monet = criterion_identity(identity_monet, real_monet)\n        loss_identity = (loss_identity_photo + loss_identity_monet) / 2\n        \n        # total loss\n        loss_generators_total = loss_GAN + 10 * loss_cycle + 5 * loss_identity\n        \n        # backward-pass\n        loss_generators_total.backward()\n        optim_generators.step()\n        \n        # limiting gradient norms - if they exceed 100, something went wrong\n        clip_grad_norm_(generator_photo2monet.parameters(), 100)\n        clip_grad_norm_(generator_monet2photo.parameters(), 100)\n        \n        \n        \"\"\" Train Discriminators \"\"\"\n        # switching models parameters so that only discriminators are trained\n        update_req_grad([discriminator_monet, discriminator_photo], True)\n        update_req_grad([generator_monet2photo, generator_photo2monet], False)\n        \n        # zero the parameters gradients\n        optim_discriminators.zero_grad()\n        \n        # sample images from 50 stored\n        fake_photo = buffer_photo.sample(num_images=batch_size).to(device)\n        fake_monet = buffer_monet.sample(num_images=batch_size).to(device)\n        \n        # making labels noisy for discriminators so that they don't prevail over generators\n        threshold = min(1, 0.85 + (1 - 0.85) * epoch / (num_epochs // 2))\n        noisy_labels_real = (torch.rand(discriminator_outputs_monet.size()) < threshold).float().to(device)\n        \n        # forward-pass + losses\n        loss_real_photo = criterion_GAN(discriminator_photo(real_photo), noisy_labels_real)\n        loss_fake_photo = criterion_GAN(discriminator_photo(fake_photo.detach()), labels_fake)\n        loss_photo = (loss_real_photo + loss_fake_photo) / 2\n        \n        loss_real_monet = criterion_GAN(discriminator_monet(real_monet), noisy_labels_real)\n        loss_fake_monet = criterion_GAN(discriminator_monet(fake_monet.detach()), labels_fake)\n        loss_monet = (loss_real_monet + loss_fake_monet) / 2\n        \n        loss_discriminators_total = loss_monet + loss_photo\n        \n        # backward-pass\n        loss_discriminators_total.backward()\n        optim_discriminators.step()\n        \n        # clipping gradients to avoid gradients explosion\n        clip_grad_norm_(discriminator_monet.parameters(), 100)\n        clip_grad_norm_(discriminator_photo.parameters(), 100)\n        \n        # updating intermediate results\n        avg_generators_loss += loss_generators_total.item()\n        avg_discriminators_loss += loss_discriminators_total.item()\n        \n    # saving intermediate results\n    avg_generators_loss /= len(dataloader)\n    avg_discriminators_loss /= len(dataloader)\n    history.update(avg_generators_loss, avg_discriminators_loss)\n    \n    # showing intermediate results\n    print(\"Epoch: %d/%d | Generators Loss: %.4f | Discriminators Loss: %.4f\"\n              % (epoch+1, num_epochs, avg_generators_loss, avg_discriminators_loss))\n    \n    # showing generated images\n    if (epoch + 1) % 10 == 0:\n        _, sample_real_photo = next(iter(dataloader))\n        \n        sample_fake_monet = generator_photo2monet(sample_real_photo.to(device)).detach().cpu()\n        \n        num_photos = min(batch_size, 5)\n        plt.figure(figsize=(20, 8))\n        for k in range(num_photos):\n            plt.subplot(2, num_photos, k + 1)\n            plt.imshow(unnorm(sample_real_photo[k]).permute(1, 2, 0))\n            plt.title('Input photo')\n            plt.axis('off')\n\n            plt.subplot(2, num_photos, k + num_photos + 1)\n            plt.imshow(unnorm(sample_fake_monet[k]).permute(1, 2, 0))\n            plt.title('Output image')\n            plt.axis('off')\n        plt.show()\n    \n    lr_sched_generators.step()\n    lr_sched_discriminators.step()\n'''","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:28.866156Z","iopub.status.idle":"2022-08-16T19:41:28.866566Z","shell.execute_reply.started":"2022-08-16T19:41:28.866355Z","shell.execute_reply":"2022-08-16T19:41:28.866393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:37.311202Z","iopub.execute_input":"2022-08-16T19:41:37.311632Z","iopub.status.idle":"2022-08-16T19:41:37.575402Z","shell.execute_reply.started":"2022-08-16T19:41:37.311597Z","shell.execute_reply":"2022-08-16T19:41:37.574535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"photo_dir = os.path.join(data_dir, 'photo_jpg')\nfiles = [os.path.join(photo_dir, name) for name in os.listdir(photo_dir)]\nlen(files)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:41.730091Z","iopub.execute_input":"2022-08-16T19:41:41.730490Z","iopub.status.idle":"2022-08-16T19:41:41.753154Z","shell.execute_reply.started":"2022-08-16T19:41:41.730457Z","shell.execute_reply":"2022-08-16T19:41:41.752181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dir = '../images'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:46.220269Z","iopub.execute_input":"2022-08-16T19:41:46.220702Z","iopub.status.idle":"2022-08-16T19:41:46.226026Z","shell.execute_reply.started":"2022-08-16T19:41:46.220668Z","shell.execute_reply":"2022-08-16T19:41:46.225134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NB: Here we use generator training mode to provide noise in the form of dropout\n\ngenerate_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nfor i in range(0, len(files), batch_size):\n    images = []\n    for j in range(i, min(len(files), i + batch_size)):\n        image = Image.open(files[j])\n        image = generate_transforms(image)\n        images.append(image)\n    real_photo = torch.stack(images, 0)\n    \n    fake_images = decoder_p2m(encoder_p2m(real_photo.to(device))).detach().cpu()\n    #fake_images = real_photo.to(device).detach().cpu()\n    for j in range(fake_images.size(0)):\n        img = unnorm(fake_images[j])\n        img = transforms.ToPILImage()(img).convert(\"RGB\")\n        img.save(os.path.join(save_dir, str(i + j + 1) + \".jpg\"))","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:42:10.063302Z","iopub.execute_input":"2022-08-16T19:42:10.063815Z","iopub.status.idle":"2022-08-16T19:42:18.098948Z","shell.execute_reply.started":"2022-08-16T19:42:10.063778Z","shell.execute_reply":"2022-08-16T19:42:18.097572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"execution":{"iopub.status.busy":"2022-08-16T19:41:28.874747Z","iopub.status.idle":"2022-08-16T19:41:28.875438Z","shell.execute_reply.started":"2022-08-16T19:41:28.875227Z","shell.execute_reply":"2022-08-16T19:41:28.875247Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
